{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNoSoRNLeO-g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Flatten, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vpg3QWpwjGq6"
   },
   "outputs": [],
   "source": [
    "max_length_encoder = 40\n",
    "max_length_decoder = 41\n",
    "target_vocab_size = 96\n",
    "input_vocab_size = 96\n",
    "\n",
    "required_chars = []\n",
    "vocabulary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQIqIEezjT6u"
   },
   "outputs": [],
   "source": [
    "for char in string.printable:\n",
    "    if 31 < ord(char) < 126:\n",
    "        required_chars.append(char)\n",
    "\n",
    "for i in range(len(required_chars)):\n",
    "    vocabulary[required_chars[i]] = i+1\n",
    "\n",
    "vocabulary['\\n'] = 95\n",
    "vocabulary['\\t'] = 96\n",
    "\n",
    "tokenizer_raw_ip = Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=False,\n",
    "    filters=None\n",
    ")\n",
    "\n",
    "tokenizer_target_ip = Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=False,\n",
    "    filters=None\n",
    ")\n",
    "\n",
    "# Using the printable 94 characters as the vocabulary\n",
    "tokenizer_target_ip.word_index = vocabulary\n",
    "tokenizer_raw_ip.word_index = vocabularycla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be8SQCf0e4Rs"
   },
   "outputs": [],
   "source": [
    "# Encoder class with Embedding layer and LSTM layer.\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, name=\"Embedding_Layer_Encoder\")\n",
    "        self.lstm_1 = LSTM(self.enc_units, recurrent_dropout=0.2, return_state=True, return_sequences=True, name=\"Encoder_LSTM_1\")\n",
    "        self.lstm_2 = LSTM(self.enc_units, recurrent_dropout=0.2, return_state=True, return_sequences=True, name=\"Encoder_LSTM_2\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        # input_embedded = self.embedding(input_sentances)\n",
    "        self.lstm_output,_,_ = self.lstm_1(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.lstm_2(self.lstm_output)\n",
    "\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n",
    "    \n",
    "# Decoder class with embedding and LSTM layer.    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "        # we are using embedding_matrix and not training the embedding layer\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, name=\"Embedding_Layer_Decoder\",)\n",
    "        self.lstm = LSTM(self.dec_units,  dropout=0.2, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "    \n",
    "    def call(self, target_sentences, state_h, state_c):\n",
    "        # target_embedded           = self.embedding(target_sentences)\n",
    "        lstm_output, _,_        = self.lstm(target_sentences, initial_state=[state_h, state_c])\n",
    "        return lstm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5LUgKLUe9j8"
   },
   "outputs": [],
   "source": [
    "# Model 1 - 1 layer LSTM model for each encoder and decoder\n",
    "class Model1(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.encoder = Encoder(vocab_size=input_vocab_size+1, embedding_dim=30, input_length=encoder_inputs_length, enc_units=512)\n",
    "        self.decoder = Decoder(vocab_size=target_vocab_size+1, embedding_dim=30, input_length=decoder_inputs_length, dec_units=512)\n",
    "        self.dense   = Dense(output_vocab_size+1, activation='softmax')\n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96klG5HYfRee"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "  word_list = []\n",
    "  split_sentence = input_sentence.split(\" \")\n",
    "  for word in split_sentence:\n",
    "\n",
    "      encoder_seq = tokenizer_raw_ip.texts_to_sequences([word])\n",
    "\n",
    "      encoder_seq = pad_sequences(encoder_seq, maxlen=max_length_encoder, dtype='int32', padding='post')\n",
    "\n",
    "      encoder_seq = tf.keras.utils.to_categorical(encoder_seq, num_classes=len(tokenizer_raw_ip.word_index.keys())+1)\n",
    "\n",
    "      enc_output, enc_state_h, enc_state_c = model.layers[0](encoder_seq)\n",
    "\n",
    "      dec_input = np.zeros((1, 1, len(tokenizer_raw_ip.word_index.keys())+1))\n",
    "\n",
    "      dec_input[0, 0, tokenizer_target_ip.word_index['\\t']] = 1.\n",
    "\n",
    "      input_state = [enc_state_h, enc_state_c]\n",
    "\n",
    "      output_word = []\n",
    "\n",
    "      for i in range(max_length_decoder):\n",
    "          # cur_emb = model.layers[1].embedding(dec_input)\n",
    "\n",
    "          predicted_out, state_h, state_c = model.layers[1].lstm(dec_input, input_state)\n",
    "\n",
    "          dense_layer_out = model.layers[2](predicted_out)\n",
    "\n",
    "          input_state = [state_h, state_c]\n",
    "      \n",
    "          output_word_index = np.argmax(dense_layer_out)\n",
    "\n",
    "          # print(output_word_index)\n",
    "\n",
    "          for key, value in tokenizer_target_ip.word_index.items():\n",
    "\n",
    "            if output_word_index == value:\n",
    "                output_word.append(key)\n",
    "\n",
    "          dec_input = np.reshape(output_word_index, (1, 1))\n",
    "\n",
    "          dec_input = np.zeros((1, 1, len(tokenizer_raw_ip.word_index.keys())+1))\n",
    "\n",
    "          dec_input[0, 0, output_word_index] = 1.\n",
    "\n",
    "\n",
    "          if output_word_index == tokenizer_target_ip.word_index['\\n']:\n",
    "            break\n",
    "\n",
    "      word = \"\".join(output_word)\n",
    "      word_list.append(word)\n",
    "\n",
    "  sentence = ''.join(word_list)\n",
    "  sentence = sentence.replace(\"\\n\", \" \")\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quGqWvPxikN8",
    "outputId": "338a0c3e-c58c-4b43-8584-527e658e1334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6f6b14ce90>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = Model1(encoder_inputs_length=max_length_encoder,decoder_inputs_length=max_length_decoder,output_vocab_size=target_vocab_size)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.load_weights(\"model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj0bzjX7ej79"
   },
   "outputs": [],
   "source": [
    "def final_fun(input_sentence):\n",
    "    output_sentence = predict(input_sentence)\n",
    "    print(\"*\"*30)\n",
    "    print(\"Input: \", input_sentence)\n",
    "    print(\"Output: \", output_sentence)\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j74LFwgeh1u9",
    "outputId": "81680765-42e1-4204-e9f3-8cc62766cc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Input:  wo aer yuo\n",
      "Output:  wou art you \n",
      "Function 1 has taken 1.5915215015411377 seconds to execute\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# input_sentence = input(\"Enter your string: \")\n",
    "input_sentence = \"wo aer yuo\"\n",
    "output = final_fun(input_sentence)\n",
    "print(\"Function 1 has taken %s seconds to execute\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQvdvh1FnHqj"
   },
   "source": [
    "# Inference\n",
    "\n",
    "From the output seen in the training, several anaylsis of the model prediction are as follows:\n",
    "\n",
    "- The model is able to do better compared to previous models since we are training the model on corrputed words and target words rather than input sentences which are longer. \n",
    "- The model achieves a good accuracy of around ~99% but note that there are several paddings due to which the accuracy is shown higher.\n",
    "\n",
    "\n",
    "- Looking at the sentences, we see that errors where the letters are exchanged or letters are replaced with another letter the model is able to correct them to a very good extent\n",
    "- The model however does not perform that well when it encouters missing letters in a word or addition of letters in the word.\n",
    "- Each sentence had maximum of 3 errors introduced in them, the is able to correct 2 errors depending on the sentence and the vocabulary.\n",
    "\n",
    "- With larger non repeating, non augmented dataset the model will perform much better covering wide variety of errors and mistakes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
