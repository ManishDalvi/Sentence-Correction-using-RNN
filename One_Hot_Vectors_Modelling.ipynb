{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w6QSvoq2wWc9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Flatten, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "EDJOwl1zw3aG",
    "outputId": "1e6f789d-22ce-4934-d028-8aa5dc587bd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ya. Next week coming.</td>\n",
       "      <td>Ya. Next week coming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah wana save n stinge... We shall eat smting...</td>\n",
       "      <td>Yes, I want to save and stinge. We shall eat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dunno how come cannot go online leh, tt fuji...</td>\n",
       "      <td>I don't know how come I cannot go online. That...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey come online? We discuss eng with regina</td>\n",
       "      <td>Can you come online? We shall discuss Eng with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ü all go then i go lor... Free one wat...</td>\n",
       "      <td>All go then I go. It is free.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input                                             target\n",
       "0                              Ya. Next week coming.                              Ya. Next week coming.\n",
       "1  Yeah wana save n stinge... We shall eat smting...  Yes, I want to save and stinge. We shall eat s...\n",
       "2    Dunno how come cannot go online leh, tt fuji...  I don't know how come I cannot go online. That...\n",
       "3        Hey come online? We discuss eng with regina  Can you come online? We shall discuss Eng with...\n",
       "4          Ü all go then i go lor... Free one wat...                      All go then I go. It is free."
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_2.csv')\n",
    "test = pd.read_csv('test_2.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tm0tj7yC4tZw"
   },
   "outputs": [],
   "source": [
    "input_lengths = []\n",
    "target_lengths = []\n",
    "for i, row in train.iterrows():\n",
    "  input_lengths.append(len(row['input']))\n",
    "  target_lengths.append(len(row['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "AhCYQVkV5IBh",
    "outputId": "0868745d-6411-4a1e-ebca-56b74e6a2885"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_value = range(len(input_lengths))\n",
    "y_value = sorted(input_lengths)\n",
    "plt.plot(x_value , y_value)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3PiO7l6J5Id-",
    "outputId": "4b7ff909-0ba1-4e5d-b21c-2ac6b6c3bb75"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnIQQIO4RFtgCCiBtoVCyKu7jVra69dW/B+1NrN63VWttq77W3brfXumBd24po3RW3WsUVJCyyKbIvIUAI+5b18/tjTsKEJUxCzpxJ8n4+Hnlk5jtn5rxzMplPzjnf8/2auyMiIhIvLeoAIiKSelQcRERkFyoOIiKyCxUHERHZhYqDiIjsolnUAfZF586dPScnJ+oYIiINypQpU9a4e3ZNyzTo4pCTk0NeXl7UMUREGhQzW7K3ZXRYSUREdqHiICIiu1BxEBGRXag4iIjILkIrDmbWy8w+NLM5ZjbbzG4K2jua2ftmNi/43iFoNzP7s5nNN7MZZnZ4WNlERKRmYe45lAE/d/fBwDDgejMbDNwKfODuA4APgvsAZwADgq9RwCMhZhMRkRqEVhzcvcDdpwa3NwFfAz2Ac4FngsWeAc4Lbp8LPOsxE4H2ZtY9rHwiIrJnSTnnYGY5wFBgEtDV3QuCh1YCXYPbPYBlcU9bHrTt/FqjzCzPzPIKCwtDyywikqqe/HQR42cW7H3BfRB6cTCz1sBLwE/cfWP8Yx6bTKJWE0q4+xh3z3X33OzsGi/wExFplB77eAEffL061HWEWhzMLINYYfiHu78cNK+qPFwUfK/8CfOBXnFP7xm0iYhIYPWm7azaWEy/7KxQ1xNmbyUDngC+dvf74x56HbgyuH0l8Fpc+xVBr6VhwIa4w08iIgJMWrgWgL6dwy0OYY6tNBy4HJhpZtODttuAe4AXzOxaYAlwcfDYeOBMYD6wFbg6xGwiIg3Sms3FABzdt2Oo6wmtOLj7p4Dt4eGTd7O8A9eHlUdEpDFYs7mY9DSjQ6vmoa5HV0iLiDQghZuK6ZTVnLS0Pf3vXT9UHEREGpB3Z68iu01m6OtRcRARaSBWrN/Ghm2lZDUPfyoeFQcRkQZi/urNAPz01IGhr0vFQUSkgVhQGCsO+3dpHfq6VBxERBqID+cW0qZFMzq3DrenEqg4iIg0GHNWbKRdywxi1xiHS8VBRKQBWLelhDWbi7noiF57X7geqDiIiDQAH86NDUN3QLfwzzeAioOISIOwuGgrACcO6pKU9ak4iIg0AG/NWEHvjq3IbJaelPWpOIiIpLiCDdtYULglKVdGV1JxEBFJcW9+FZu94KenhH/xWyUVBxGRFObu/GH81wActF/bpK1XxUFEJIXlLVkHwGVH9aJDVvgXv1VScRARSWHPTVoKwHXH90/qesOcJvRJM1ttZrPi2saZ2fTga3HlDHFmlmNm2+IeezSsXCIiDcm7s1fSPD2NPp3CnRZ0Z2GO+/o08BDwbGWDu19SedvM7gM2xC2/wN2HhJhHRKRBWVK0ha0l5Zx9aPekrzvMaUI/NrOc3T1msYFBLgZOCmv9IiIN3TuzVgJwxTE5SV93VOccjgNWufu8uLa+ZjbNzCaY2XF7eqKZjTKzPDPLKywsDD+piEhEpixZhxkcmdMh6euOqjhcBoyNu18A9Hb3ocDPgOfMbLd9ttx9jLvnuntudnZ2EqKKiCRfWXkF781ZRb/OWUkZhXVnSS8OZtYMuAAYV9nm7sXuXhTcngIsAJJ3tYeISIp5IW85AKcM7hrJ+qPYczgF+Mbdl1c2mFm2maUHt/sBA4CFEWQTEUkJz0+OdWG96eQBkaw/zK6sY4EvgAPMbLmZXRs8dCnVDykBjABmBF1b/wlc5+5rw8omIpLKisvKmbF8Awd0bUOr5mF2Kt2zMHsrXbaH9qt20/YS8FJYWUREGpKHP1wAwMVHJmdin93RFdIiIilk4/ZSHvs4Vhyu+k5OZDlUHEREUsS2knIufvQLtpdWMPKgrqSnJb+XUiUVBxGRFHH7KzP5ZuUmenVsyb0XHRZpFhUHEZEUsGjNFsbPKsAMXr/+WNq0yIg0TzSnwUVEpMrclZsY+eDHAFx/Yv+kDs29JyoOIiIRWrelhIse/RyA/77gEM4f2iPiRDEqDiIiEdm0vZST7vuIjdvLGNq7PZce2SuSoTJ2R+ccREQi4O5c8PDnrNtaytDe7Xn6qqNSpjCA9hxERCLx7BdLmLd6M307Z/G3a4+mdWZqfRxrz0FEJMk2bCvlztdnA/DP645JucIAKg4iIkn3vUdiJ6BHH9+PTq0zI06zeyoOIiJJNHvFBuav3kyvji35+akHRB1nj1QcRESSpKy8gk/mrQHgocsOp3mz1P0ITr0DXSIijdDqjdu5771vGZe3jJYZ6Rzco13UkWqk4iAiErJ3Z69k9N+mAHDQfm25+7yDIx1ULxEqDiIiIXL3qsLwx+8dwrB+nejTKSviVHun4iAiEqKnP18MwHcP249LjuwdbZhaCHOa0CfNbLWZzYpr+62Z5ZvZ9ODrzLjHfmVm881srpmNDCuXiEiyzFu1id+9MQeIjZvUkIR5qvxp4PTdtD/g7kOCr/EAZjaY2NzSBwXPedjM0kPMJiISKnevGmn1rnMPSskL3WoSWnFw94+BtQkufi7wvLsXu/siYD5wVFjZRETClr9+GxUOIwZm84NhfaKOU2tRdLK9wcxmBIedOgRtPYBlccssD9p2YWajzCzPzPIKCwvDzioiUidn/9+nAFw3ol9KDaiXqGQXh0eA/sAQoAC4r7Yv4O5j3D3X3XOzs7PrO5+IyD776ycLWb+1lIP2a8vR/TpFHadOkloc3H2Vu5e7ewXwODsOHeUDveIW7Rm0iYg0KGu3lHD3W18D8Pdrj0756xn2JKnFwcy6x909H6jsyfQ6cKmZZZpZX2AA8GUys4mI7Ct35+63Yr2Tbh55QEpM91lXoZ0+N7OxwAlAZzNbDtwJnGBmQwAHFgOjAdx9tpm9AMwByoDr3b08rGwiImH43RtzeHlq7KDHdcf3jzjNvgmtOLj7ZbtpfqKG5f8A/CGsPCIiYaqo8KoL3l6/YXiDPZxUKXWHBBQRaSDcnX9OWQ7E5mg4tGf7iBPtu4Z1VYaISAq66qnJTPg21rX++hP3jzhN/VBxEBHZBw/9ex4Tvi1kWL+OjB7Rn7YtMqKOVC9UHERE6mhJ0Rbufe9bAO6/eAj7tW8ZcaL6o3MOIiJ1dNebsesZ/nThoY2qMICKg4hInYyfWcC/vl5Fm8xmXHhEz6jj1DsdVhIRqaX/Gv81f/1kIQBjRw1rkGMn7Y2Kg4hILdz84le8OGU5+3dpzc0jD0j5uaDraq+HlczsIjNrE9z+tZm9bGaHhx9NRCS1fDZ/DS8G1zM8eMkQRh7ULeJE4UnknMMd7r7JzI4FTiF2lfMj4cYSEUk9P3/hKwDevum4RrvHUCmR4lA5xtFZwBh3fwtouKNJiYjUQVl5BWs2FzOkV3sO7N426jihS6Q45JvZY8AlwHgzy0zweSIijcZNz0+nrML5/tG9o46SFIl8yF8MvAuMdPf1QEfg5lBTiYikkHGTl/LWzAKapRkjBzfe8wzxauytZGbpwFR3H1TZ5u4FxGZxExFp9NZsLuaXL80E4M0fH0u7Vo1jeIy9qXHPIZhTYa6ZNY39KBGROIvXbCH37n8B8JNTBjCoW+M/11ApkescOgCzzexLYEtlo7ufE1oqEZGIFW4q5oR7PwLg3CH7cUMjGW01UYkUhzvq8sJm9iRwNrDa3Q8O2v4EfBcoARYAV7v7ejPLAb4G5gZPn+ju19VlvSIi++red+fy3JdLATj9oG48eMmQRnkVdE32ekLa3ScQm9IzI7g9GZiawGs/DZy+U9v7wMHufijwLfCruMcWuPuQ4EuFQUQi8ddPFvLQh/Np1zKD0cf346HvD21yhQES2HMwsx8Bo4j1UuoP9AAeBU6u6Xnu/nGwRxDf9l7c3YnAhbWLKyISns/nr+Hut2Ijrd570aEc0adjxImik0hX1uuB4cBGAHefB3Sph3VfA7wdd7+vmU0zswlmdtyenmRmo8wsz8zyCgsL6yGGiAiUlFXw/b9OAuCZa45q0oUBEisOxe5eUnnHzJoBvi8rNbPbgTLgH0FTAdDb3YcCPwOeM7Pddgtw9zHunuvuudnZ2fsSQ0Skyk/GTQPglAO7cvxAfbYkUhwmmNltQEszOxV4EXijris0s6uInaj+D3d3AHcvdvei4PYUYierB9Z1HSIitfGzcdMZP3MlZrHDSZJYcbgVKARmAqOB8cCv67IyMzsduAU4x923xrVnBxfcYWb9gAHAwrqsQ0SkNqYvW8/L0/JJM3jjhmNp30pDx0ECJ6TdvcLMngEmETucNLfyP/6amNlY4ASgs5ktB+4k1jspE3g/OPtf2WV1BPB7MysFKoDr3H1t3X4kEZHELFqzhfP+8hkAb/34uCYxoF6iEumtdBax3kkLACN24ni0u79d0/Pc/bLdND+xh2VfAl7ae1wRkX23cXspP3w6j2XrYgcwrjimjwrDThK5CO4+4ER3nw9gZv2Bt6je00hEpEFYtXE7p9w/gU3byzimXyfOPrQ7t515YNSxUk4ixWFTZWEILAQ2hZRHRCQU20vLGf23KSxas4VN28s4qm9Hnrr6SFpkpEcdLSXtsTiY2QXBzTwzGw+8QOycw0XErpIWEWkQ5q/ezE/HTWdm/gaG9m7P8P07c9e5B9EsXVPT7ElNew7fjbu9Cjg+uF0ItAwtkYhIPbrrzTm8O3sly9dt46xDuvNf5x/SZIbd3hd7LA7ufnUyg4iI1Ldb/vkVL+Qtp2/nLK4Z3pc7zj6wSY6TVBeJ9FbqC9wI5MQvryG7RSSV/frVmbyQtxyA/7tsKAf3aBdxooYlkRPSrxLrgvoGsWsQRERS1vbScm4cO43356wC4MNfnEDfzlkRp2p4EikO2939z6EnERHZR+UVzoWPfs6s/I0M6dWeu887WIWhjhIpDv9rZncC7wHFlY3unsicDiIiSbG9tJxLx0xkVv5G2rXM4Nlrj6JtC514rqtEisMhwOXASew4rOTBfRGRyP3lw/m8N2cVXy1bT+fWmbz142NVGPZRIsXhIqBf/LDdIiKpoKy8gltemsGbMwro2Ko5Jw3qwoOXDlFhqAeJFIdZQHtgdchZREQS9t7slfz100V8uWgtA7u25q5zD+bofp2ijtVoJFIc2gPfmNlkqp9zUFdWEYnEwx/N56nPFrOtpJzh+3figUuG0KVNi6hjNSqJFIc7Q08hIpKgr5at53/emUuXNpn88oxBXD6sT9SRGqVE5nOYkIwgIiJ78/mCNXz/8dg8z/932VAdRgpRIldIb2LHnNHNgQxgi7tr8HMRSZq/T1zCIx8tAGD0iH4qDCHb65CE7t7G3dsGxaAl8D3g4URe3MyeNLPVZjYrrq2jmb1vZvOC7x2CdjOzP5vZfDObYWaH1/FnEpFG5rXp+fz61VlsLSnjF6cN5FeafyF0tRqv1mNeBUYm+JSngdN3arsV+MDdBwAfBPcBziA2d/QAYBTwSG2yiUjj9MHXq7jp+ekA3HvRYdxw0oCIEzUNiRxWuiDubhqQC2xP5MXd/WMzy9mp+Vxic0sDPAN8BPwyaH82mJ96opm1N7Pu7l6QyLpEpHHZWlLG79+Yw4dzY73oH7xkCCcf2DXiVE1HIr2V4ud1KAMWE/sgr6uucR/4K4HK33YPYFnccsuDtmrFwcxGEduzoHfv3vsQQ0RSVWl5BRc/9gWz8jfSq2NLfnfOQZw3tEfUsZqURHorhTavg7u7mfnel6z2nDHAGIDc3NxaPVdEUt/zXy7lvTmrmJW/kTaZzXjnphFkZSbyf6zUp0QOK2UDP2LX+RyuqeM6V1UeLjKz7uy48jof6BW3XM+gTUSagOXrtvLnD+bx5owC0tOMg3u05Zmrj1JhiEgiW/014BPgX0B5PazzdeBK4J7g+2tx7TeY2fPA0cAGnW8Qafw2F5fxx7e/YWb+BqYvW09Op1bccfZgnV+IWCLFoZW7/7IuL25mY4mdfO5sZsuJXW19D/CCmV0LLAEuDhYfD5wJzAe2ApqmVKSRm7iwiIc/WsDH3xbSrW0LTh3clcevyI06lpBYcXjTzM509/G1fXF3v2wPD528m2UduL626xCRhmnm8g1cOmYizdPTOKxnO8aNPoYWGelRx5JAIsXhJuA2MysGSgEj9lmuK6RFpFYKNxXz0L/nUVJeweTF6wC48aT9ufFkXbuQahLprdQmGUFEpHH78JvVjPl4IV8sLCK7TSYGXHtsXxWGFKVuACISqhnL1/Ni3nLemb2SDdtKOapvR8aNGoaZRR1NaqDiICKhmb96E+c89BnpaUaHVhnccfZgDbHdQKg4iEi9m7dqE2O/XMaXi4sAuGZ4DrefNTjiVFIbCRUHMzsWGODuTwUXxbV290XhRhORhmb8zALyFq/jy8VFzF4Ru8L5wiN6qjA0QIlcIX0nscH2DgCeIjafw9+B4eFGE5GGoKy8gkc+WsCGbaWMm7yM4rIKMpulceHhPfnTRYdFHU/qKJE9h/OBocBUAHdfYWbqwSQiTFpYxLi8Zbw8NZ8WGWlkpKfxwCVDOOvQ7lFHk32USHEoiR8gz8yyQs4kIg3AR3NXc9VTk0kz6Na2BW/++Fg6t86MOpbUk0SKwwtm9hjQ3sx+BFwDPB5uLBFJVa9MW868VZsZNzk2wv4tpw/iuuP7R5xK6lsiF8Hda2anAhuJnXf4jbu/H3oyEUkpy9Zu5cW8ZTz04XwAmqWncc8Fh3DpUZpXpTFKqLeSu79vZpMqlzezju6+NtRkIpISZixfz7/mrOLzBUXkLVlHi4w0nrzqSL7Tv3PU0SREifRWGg38jtjUoBUEYysB/cKNJiJRKi2v4OnPFjN28lIWFm7BDEYe1JXHLteoqU1BInsOvwAOdvc1YYcRkdQwefFa/pm3nHF5sfMK/++E/txy+qCIU0kyJVIcFhCbX0FEGrnJi9cyaWERz01ayooN2+mY1ZwPf3EC7VpmRB1NkiyR4vAr4PPgnENxZaO7/zi0VCKSNEuLtvLmzBW4w98nLqFgw3YAbj/zQH54XF8NkNdEJVIcHgP+Dcwkds5hn5jZAcC4uKZ+wG+A9sTmqi4M2m+rywRDIpKY4rJyxk5ayhszCpiyZF1V++1nHshVw3PISE+LMJ1ELZHikOHuP6uvFbr7XGAIgJmlA/nAK8SmBX3A3e+tr3WJyK6WFm3lndkFLFqzhbFfxs4pnHVId+6/5DAMo3kzFQVJrDi8bWajgDeoflipPrqyngwscPcl2nUVCZe78/LUfP4xaQlTl64HoEOrDD74+Ql0aJWhw0dSTSLFoXIe6F/FtdVXV9ZLgbFx928wsyuAPODn7r5u5ycEhWoUQO/euvhGZG/cnVem5bOwcEvVBWxnHdqdP114KBnpaTp8JLtl7h7Nis2aAyuAg9x9lZl1BdYQKzx3Ad3d/ZqaXiM3N9fz8vLCDyvSALk7r07PZ1HhFv7871hRaJ6exus3DueArm20p9CEmdkUd6/xgpVELoLLAP4TGBE0fQQ85u6l+5jvDGCqu68CqPwerPNx4M19fH2RJqesvIKXp+WzraScos3FuxSFnE5ZtMhIjzilNASJHFZ6hNgcDg8H9y8P2n64j+u+jLhDSmbW3d0LgrvnA7P28fVFmoyy8gpenb6Crws28sSnO+bhykg3Xr/hWHI6ZdGyuYqCJC6R4nCku8fP2PFvM/tqX1YaDPt9KjA6rvl/zGwIscNKi3d6TER2o7S8gje+WsHsFTuKQlbzdN75yQiyMpvRIiONVs01G7DUXiLvmnIz6+/uCwDMrB9Qvi8rdfctQKed2i7fl9cUaSq2lZTz1swCSsoqWFC4uVpRePumEXRu01wFQfZZIu+gm4EPzWwhsUH3+hC7JkFEkmhW/gZmr9jA9GXrq65PAGid2Yy3gol2sjJVFKR+JDKfwwdmNoDYXA4Ac929uKbniMi+21pSxruzV1JaHutReP9737JyY2xoi86tM3nzxmMxixUHFQWpb4n0VroIeMfdZ5jZr4HDzexud58afjyRpmdJ0RYmL17H5EVrq0ZFrfTTUwZyUW5P2rXMUEGQUCXy7rrD3V80s2OJXdF8L7HeSkeHmkykCVmxfhsTFxYB8MSni5i9YiMA3du14MXrjgEgzYzu7Vro+gRJioROSAffzwIed/e3zOzuEDOJNBkFG7YxaeFa/jFpCZMX7xgQ4OLcntx40gA6ZDWntfYQJAKJvOvyzewxYl1P/2hmmYCutxfZB9OWrmPZum08+/li8oIRUc84uBu3nhGbUKdnh1akp2kPQaKTSHG4GDgduNfd15tZd2I9mEQkAUuKtjAzf0PVfXf4+YtfUVIWGwH/zEO6cfPIQfRo31IjokrKSKS30lbg5bj7BUDBnp8hIuu3lvDFgiIceOD9b5m3evMuy9x93sEM69eJ3h1bqShIytHBTJF6UlpewSfzCikpq+CVafm8O7tquDAuH9aHK47pU3W/ebM0endspZPLkrJUHETqqKSsgs/mr6GkPHZ4aPqy9Tzy0YKqx4/o04H/Ov8QzKBf5yyaaWhsaUBUHEQStGl7KZMWrqVykPu8JWt5bMLCasu0yWzG2FHDSE8zenVspZ5G0mDpnStSg/IKZ+LCIorLynll2gre+GpFtcfbtmjGcz8aRuXRoezWmXRp2yKCpCL1S8VBZDe2lZQzefFapi1dzwP/+raq/ZAe7fjvCw6put+ljYqBNE4qDiKB1Zu283XBJgDenlnA85NjQ1e0zEjn7z88mmZpRp9OrWjfqnmUMUWSQsVBhFjX0zMe/ISiLSVVbf2ys7jvosPIbpNJzw6tIkwnknwqDtJkFW4q5ttVsT2FRycsoGhLCd89bD+u+k4OAL07tiK7TWaECUWiE1lxMLPFwCZiYzeVuXuumXUExgE5xGaDu9jd1+3pNURqa9GaLazcEBv2+p53vuGrZeurHhvQpTV//N4hmihHhOj3HE509zVx928FPnD3e8zs1uD+L6OJJo3B+q0lzF0Z2zsod+eapyezvbSi6vFzDtuPHwyLXZw2qHsbFQaRQKr9JZwLnBDcfgb4CBUHqQV3Z/aKjWwrjQ0mfN97c5m4cG21ZW45/QCG9uqAGRzas50KgshuRPlX4cB7ZubAY+4+BugajN0EsBLoGlk6aRAqi0FxWawYfLNyE7e/MqvaMqcc2JVrhucAkJmRxpBeHTTiqcheRFkcjnX3fDPrArxvZt/EP+juHhSOasxsFDAKoHfv3slJKimluKycOSs24sDsFRu549XqxSA9zXj0B0fQMiMdgEN7taNti4wIkoo0XJEVB3fPD76vNrNXgKOAVWbW3d0LgqHBV+/meWOAMQC5ubm7FA9pnErLK/imYBPl7oybvIyxXy6teqxZUAwqRzbNbpPJgd3bRhVVpFGIpDiYWRaQ5u6bgtunAb8HXgeuBO4Jvr8WRT6JXml5BXNXbsKD8v/GjBWM+XjHOEYDu7bmtjMPBKBLmxYM3k/FQKQ+RbXn0BV4JRiuuBnwnLu/Y2aTgRfM7FpgCbGJhqQJKC2vYN6qzVQE1eDVafn89dNF1Zbp0b4ld593MAADu7WhR/uWSc8p0lREUhzcfSFw2G7ai4CTk59Ikq20vIL5qzdX7Rm8OGUZT322uNoyvTq25M6zD6q6v3+X1uR0zkpiSpGmS334JGncncVFWykuK2fspKU888WSao/ndGpVdagIYGDXNioGIhFRcZDQFJeVs6Roa9X9aUvX8cuXZlbd75+dxc0jB1XdH9RNxUAkVag4SL3aXlrO8nXbAHjwX9/y5ozq041npBv3XzyEZmnG4P3a0qeTioFIKlJxkH1SVl7B0rU79g5+89psPp2/Y0SUw3u359pj+1Xd3699C4b27pDUjCJSeyoOUmsVFc6ydVtxh4c/ms8LecurPX78wGy+d0RPAI7M6UD3dupVJNLQqDjIXlVUOPnrt1X1LHr2i8XVupkO7t6W0cfv2DsYvn9nOrfWUNciDZmKg+yiosJZsWFHMXji00U8/fniasv07ZzFj0/eH4AjenekdydNhiPSmKg4CAAbtpayuaQMgMcmLODZnbqZ9s/O4v+dsH/V/cN6tWP/Lm2SmlFEkkfFoYnaXlrO+q2lAKzZXMx5f/mMsoodQ1UN7NqaHx2341DR0N4d2L9L66TnFJFoqDg0MeUVTtHmYq5+ejKzV2ys9tjPTh1It7YtADgipwP9s1UMRJoqFYcmYEtxGVtLYvMd3Pn6LMbPXAnA6Qd14/gDsgFo2yKDMw/pRjDelYg0cSoOjdSGbaWUlVewflspZ/7vJxSX7ZgaM7dPBy48oienH9yN9q2aR5hSRFKVikMjsaW4jJKgAHy2YA03PDet2uPXHd+fHh1i1xuceEA2PTuod5GI7JmKQwPl7mzcHutdtGjNFr73yOeUx51Qzkg3fn3WYMwgq3kzzh/agzRNjSkiCVJxaCCKy8qr9gwA7n//212GuP7FaQNpnRn7lfbNbs3xA7OTGVFEGhEVhxRVVl5RdZ5g/bZSTrt/AluCk8qVBnZtzSVHxubRzm6TyTmH7Zf0nCLSOKk4pIjtpeVVVyRXuHPaAx+Tv35btWWuHp5Tbfaz4wZkc0A3XYgmIvUv6cXBzHoBzxKbKtSBMe7+v2b2W+BHQGGw6G3uPj7Z+ZKlpKyiakrM16bnV5vnoNK5Q/bjoGBu5KzMZlx6ZG/Sdd5ARJIgij2HMuDn7j7VzNoAU8zs/eCxB9z93ggyha6iwikPisHkRWu5/Mkvq51Abt8qg+uO7191v3l6Ghcf2avqHIKISDIl/ZPH3QuAguD2JjP7GuiR7BzJUFHhOLFDRife+xGrNxVXPWYGN488gMprzg7r2Z7h+3eOJqiIyE4i/bfUzHKAocAkYDhwg5ldAeQR27tYt5vnjAJGAfTu3TtpWRPhvmNPYMbyDVz06BeUlO/oYXTB0B70y47NfJbTOYuzD9UJZBFJTRb/gZbUFZu1BiYAf3D3l82sK7CG2HmIu4Du7n5NTa+Rm5vreXl54YdNwOqN29lfkAkAAAiZSURBVDn5/glsCq49qHTjSfuTkZ5Gy4x0Lj+mDy0y0iNKKCISY2ZT3D23pmUi2XMwswzgJeAf7v4ygLuvinv8ceDNKLLVxdOfLeJ3b87BHX4wrHfVRDc9O7TiwmBGNBGRhiSK3koGPAF87e73x7V3D85HAJwPzEp2ttp4eepybvnnDCrcqXDo2jaTUSP6c/V3cnQlsog0eFHsOQwHLgdmmtn0oO024DIzG0LssNJiYHQE2RJyx6uz+PukJbRrmcHlw/oAcEz/Tnynv04oi0jjEEVvpU+B3f1rnfLXNGzYWsrIBz9m5cbtDOrWhl+ePogTB3WJOpaISL1TJ/oE3fDcVN6bs4qSsgouGNqDUcf3Y1C3tlHHEhEJhYrDXryYt4zfvTGHzcVlDOnVnpMHdWH08f1p3iwt6mgiIqFRcdiDF/KWcdcbc9hWWk6bFs0YPaIflx3Vm5zOWVFHExEJnYrDblRUOLf8cwYdWmVw5ZE5HNOvE6cM7hp1LBGRpFFx2MkXC4q4cexUAE4c1IU7zh4ccSIRkeRTcYhTtLmYyx6fCMCoEf34z7iB8EREmhIVh8DWkjK+c8+/AbjsqF7cduaBEScSEYmOutwE3p29kuKyCob0as/vzz046jgiIpFScQi8mLccgGevPYqMdG0WEWna9CkIPPXZIj5fUETPDi1p2yIj6jgiIpFr8sVh0sIifvfGHAAeuGRIxGlERFJDky8OH86NTVn95FW5HJnTMeI0IiKpoUkXh7VbSnh0wgL279KakwbpIjcRkUpNujhMXRKbhfQ0Xf0sIlJNky4Of/loPgCjR+hiNxGReE3yIrhvVm7kxuemsWjNFrq2zaRdK/VQEhGJl3J7DmZ2upnNNbP5ZnZrGOto0SydAV1bM/Lgbjx2eY1zbIuINEkptedgZunAX4BTgeXAZDN73d3n1Od6cjpn8fB/HFGfLyki0qik2p7DUcB8d1/o7iXA88C5EWcSEWlyUq049ACWxd1fHrRVMbNRZpZnZnmFhYVJDSci0lSkWnHYK3cf4+657p6bnZ0ddRwRkUYp1YpDPtAr7n7PoE1ERJIo1YrDZGCAmfU1s+bApcDrEWcSEWlyUqq3kruXmdkNwLtAOvCku8+OOJaISJOTUsUBwN3HA+OjziEi0pSl2mElERFJAebuUWeoMzMrBJbsw0t0BtbUU5z6lKq5QNnqIlVzgbLVRarmgsSz9XH3Grt7NujisK/MLM/dU278jFTNBcpWF6maC5StLlI1F9RvNh1WEhGRXag4iIjILpp6cRgTdYA9SNVcoGx1kaq5QNnqIlVzQT1ma9LnHEREZPea+p6DiIjshoqDiIjsokkWh2TMNlfDunuZ2YdmNsfMZpvZTUH7b80s38ymB19nxj3nV0HWuWY2MuR8i81sZpAhL2jraGbvm9m84HuHoN3M7M9BthlmdniIuQ6I2zbTzWyjmf0kqu1mZk+a2WozmxXXVuvtZGZXBsvPM7MrQ8r1JzP7Jlj3K2bWPmjPMbNtcdvu0bjnHBG8D+YH2S2kbLX+/YXx97uHbOPici02s+lBe9K2Ww2fF+G/19y9SX0RG7NpAdAPaA58BQxO4vq7A4cHt9sA3wKDgd8Cv9jN8oODjJlA3yB7eoj5FgOdd2r7H+DW4PatwB+D22cCbwMGDAMmJfF3uBLoE9V2A0YAhwOz6rqdgI7AwuB7h+B2hxBynQY0C27/MS5XTvxyO73Ol0FWC7KfEdI2q9XvL6y/391l2+nx+4DfJHu71fB5Efp7rSnuOUQ625y7F7j71OD2JuBrdprQaCfnAs+7e7G7LwLmE/sZkulc4Jng9jPAeXHtz3rMRKC9mXVPQp6TgQXuXtPV8aFuN3f/GFi7m3XWZjuNBN5397Xuvg54Hzi9vnO5+3vuXhbcnUhsKPw9CrK1dfeJHvtkeTbuZ6nXbDXY0+8vlL/fmrIF//1fDIyt6TXC2G41fF6E/l5risVhr7PNJYuZ5QBDgUlB0w3BruCTlbuJJD+vA++Z2RQzGxW0dXX3guD2SqBrRNkqXUr1P9RU2G5Q++0URcZriP1nWamvmU0zswlmdlzQ1iPIkqxctfn9RbHNjgNWufu8uLakb7edPi9Cf681xeKQEsysNfAS8BN33wg8AvQHhgAFxHZjo3Csux8OnAFcb2Yj4h8M/iOKrP+zxeb5OAd4MWhKle1WTdTbaXfM7HagDPhH0FQA9Hb3ocDPgOfMrG2SY6Xk728nl1H9n5Gkb7fdfF5UCeu91hSLQ+SzzZlZBrFf9D/c/WUAd1/l7uXuXgE8zo5DIEnN6+75wffVwCtBjlWVh4uC76ujyBY4A5jq7quCnCmx3QK13U5Jy2hmVwFnA/8RfJgQHLIpCm5PIXYsf2CQIf7QU2i56vD7S+rv1cyaARcA4+IyJ3W77e7zgiS815picYh0trng+OUTwNfufn9ce/yx+vOByl4TrwOXmlmmmfUFBhA76RVGtiwza1N5m9iJzFlBhsreDVcCr8VluyLoITEM2BC3qxuWav/FpcJ2i1Pb7fQucJqZdQgOp5wWtNUrMzsduAU4x923xrVnm1l6cLsfsW20MMi20cyGBe/XK+J+lvrOVtvfX7L/fk8BvnH3qsNFydxue/q8IBnvtX05k95Qv4id0f+WWMW/PcnrPpbYLuAMYHrwdSbwN2Bm0P460D3uObcHWedSD71GasjWj1jvj6+A2ZXbBugEfADMA/4FdAzaDfhLkG0mkBvytssCioB2cW2RbDdiBaoAKCV2/PbaumwnYucA5gdfV4eUaz6x482V77dHg2W/F/yepwNTge/GvU4usQ/qBcBDBKMphJCt1r+/MP5+d5ctaH8auG6nZZO23djz50Xo7zUNnyEiIrtoioeVRERkL1QcRERkFyoOIiKyCxUHERHZhYqDiIjsQsVBRER2oeIgIiK7+P+kqVVX+6aX5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_value = range(len(target_lengths))\n",
    "y_value = sorted(target_lengths)\n",
    "plt.plot(x_value , y_value)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJfz3D3E5-c3"
   },
   "outputs": [],
   "source": [
    "train['input_lengths'] = input_lengths\n",
    "train['target_lengths'] = target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeqHauAI56yH"
   },
   "outputs": [],
   "source": [
    "train = train[train['input_lengths'] < 500]\n",
    "train = train[train['target_lengths'] < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ17P68b77Ln"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['input_lengths', 'target_lengths'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqnRGnG88LYy",
    "outputId": "4ae3cdd1-f95f-4831-c516-ebfc950f7c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', ' ']\n"
     ]
    }
   ],
   "source": [
    "required_chars = []\n",
    "for char in string.printable:\n",
    "  if ord(char) > 31 and ord(char) < 126:\n",
    "    required_chars.append(char)\n",
    "\n",
    "\n",
    "print(len(required_chars))\n",
    "print(required_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xWM0cCmi7s5D"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of chars and index value from 1. 0 is reserved for padding by the tokenizer.\n",
    "vocabulary = dict()\n",
    "for i in range(len(required_chars)):\n",
    "  vocabulary[required_chars[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WgItyynh7uWb"
   },
   "outputs": [],
   "source": [
    "# Use \\t as Start of Sentence and \\n as End of Sentence\n",
    "vocabulary['\\n'] = 95\n",
    "vocabulary['\\t'] = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a4nfKCXvyDC2"
   },
   "outputs": [],
   "source": [
    "# Characters that were found in train and test set and replaced with the normal english characters.\n",
    "replacements = {'£':'', 'É': 'E', 'Ñ': 'N', 'Ü': 'U', 'à': 'a', 'ä': 'a', 'å': 'a', 'è': 'e', 'é': 'e', 'ì': 'i', 'ñ': 'n', 'ò': 'o', 'ö': 'o', 'ø': 'o', 'ù': 'u', 'ü': 'u',  '“': '\"',  '”': '\"',   '，': ',',   '？': '?' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2Oq7MzJqyKa2"
   },
   "outputs": [],
   "source": [
    "for old_char, new_char in replacements.items():\n",
    "  train = train.replace(old_char, new_char, regex=True)\n",
    "  test = test.replace(old_char, new_char, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSmesdxqcuCz"
   },
   "outputs": [],
   "source": [
    "# gen_sen_train = []\n",
    "# target_sen_train = []\n",
    "# gen_sen_test = []\n",
    "# target_sen_test = []\n",
    "\n",
    "# from textblob import TextBlob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# for i, row in tqdm(train.iterrows()):\n",
    "#     textBlb = TextBlob(row['input'])\n",
    "#     textCorrected = textBlb.correct() \n",
    "#     gen_sen_train.append(str(textCorrected))\n",
    "#     target_sen_train.append(row['target'])\n",
    "\n",
    "# for i, row in tqdm(test.iterrows()):\n",
    "#     textBlb = TextBlob(row['input'])\n",
    "#     textCorrected = textBlb.correct() \n",
    "#     gen_sen_test.append(str(textCorrected))\n",
    "#     target_sen_test.append(row['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGCYuhB_c1gd"
   },
   "outputs": [],
   "source": [
    "# train = pd.DataFrame({\n",
    "#     'input':gen_sen_train,\n",
    "#     'target':target_sen_train\n",
    "# })\n",
    "\n",
    "# test = pd.DataFrame({\n",
    "#     'input':gen_sen_test,\n",
    "#     'target':target_sen_test\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CZSJAoC9dkTd"
   },
   "outputs": [],
   "source": [
    "train['input'] = train['input'].replace({'..':'.', '...':'.', '???':'?', '??':'?', '#':'', \"!!\":\"!\",\"!!!\":\"!\",\"@\":\"\",\"$\":\"\",\"%\":\"\",\"^\":\"\",\"&\":\"\",\"(\":\"\",\")\":\"\",\"_\":\"\", \";\":\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eHD0q5DDdRfK"
   },
   "outputs": [],
   "source": [
    "for i, row in train.iterrows():\n",
    "    value = row['input']\n",
    "    repl_value = re.sub(r'\\.+', \".\", value)\n",
    "    repl_value = re.sub(r'\\!+', \"!\", repl_value)\n",
    "    repl_value = re.sub(r'\\ +', \" \", repl_value)\n",
    "    train.at[i,'input'] = repl_value\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    value = row['input']\n",
    "    repl_value = re.sub(r'\\.+', \".\", value)\n",
    "    repl_value = re.sub(r'\\!+', \"!\", repl_value)\n",
    "    repl_value = re.sub(r'\\ +', \" \", repl_value)\n",
    "    test.at[i,'input'] = repl_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "R2UWkIQjS9s7",
    "outputId": "4d86f97e-47c4-4bac-fc88-7e395e936893"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ya. Next week coming.</td>\n",
       "      <td>Ya. Next week coming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah wana save n stinge. We shall eat smting g...</td>\n",
       "      <td>Yes, I want to save and stinge. We shall eat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dunno how come cannot go online leh, tt fuji.</td>\n",
       "      <td>I don't know how come I cannot go online. That...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey come online? We discuss eng with regina</td>\n",
       "      <td>Can you come online? We shall discuss Eng with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U all go then i go lor. Free one wat.</td>\n",
       "      <td>All go then I go. It is free.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input                                             target\n",
       "0                              Ya. Next week coming.                              Ya. Next week coming.\n",
       "1  Yeah wana save n stinge. We shall eat smting g...  Yes, I want to save and stinge. We shall eat s...\n",
       "2      Dunno how come cannot go online leh, tt fuji.  I don't know how come I cannot go online. That...\n",
       "3        Hey come online? We discuss eng with regina  Can you come online? We shall discuss Eng with...\n",
       "4              U all go then i go lor. Free one wat.                      All go then I go. It is free."
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BrFuNLIs2owY"
   },
   "outputs": [],
   "source": [
    "# Adding the \\t and \\n as part of start and end of sentence\n",
    "train['target_ip'] = '\\t' + train['target'].astype(str)\n",
    "train['target_op'] =  train['target'].astype(str) + '\\n'\n",
    "\n",
    "test['target_ip'] = '\\t' + test['target'].astype(str)\n",
    "test['target_op'] =  test['target'].astype(str) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aG58E_dY3JaN"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['target'], axis=1)\n",
    "test = test.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "pn9P1Pl52rMl",
    "outputId": "9bb3c2ab-51fc-4c22-e43e-04c8dc173c8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target_ip</th>\n",
       "      <th>target_op</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ya. Next week coming.</td>\n",
       "      <td>\\tYa. Next week coming.</td>\n",
       "      <td>Ya. Next week coming.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah wana save n stinge. We shall eat smting g...</td>\n",
       "      <td>\\tYes, I want to save and stinge. We shall eat...</td>\n",
       "      <td>Yes, I want to save and stinge. We shall eat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dunno how come cannot go online leh, tt fuji.</td>\n",
       "      <td>\\tI don't know how come I cannot go online. Th...</td>\n",
       "      <td>I don't know how come I cannot go online. That...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey come online? We discuss eng with regina</td>\n",
       "      <td>\\tCan you come online? We shall discuss Eng wi...</td>\n",
       "      <td>Can you come online? We shall discuss Eng with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U all go then i go lor. Free one wat.</td>\n",
       "      <td>\\tAll go then I go. It is free.</td>\n",
       "      <td>All go then I go. It is free.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ...                                          target_op\n",
       "0                              Ya. Next week coming.  ...                            Ya. Next week coming.\\n\n",
       "1  Yeah wana save n stinge. We shall eat smting g...  ...  Yes, I want to save and stinge. We shall eat s...\n",
       "2      Dunno how come cannot go online leh, tt fuji.  ...  I don't know how come I cannot go online. That...\n",
       "3        Hey come online? We discuss eng with regina  ...  Can you come online? We shall discuss Eng with...\n",
       "4              U all go then i go lor. Free one wat.  ...                    All go then I go. It is free.\\n\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dccV3FWe3IbF"
   },
   "outputs": [],
   "source": [
    "train.iloc[0]['target_ip']= str(train.iloc[0]['target_ip'])+'\\n'\n",
    "train.iloc[0]['target_op']= str(train.iloc[0]['target_op'])+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93Ub9wTxyymO",
    "outputId": "bdfee973-9c80-48d6-8b20-0e1be6a5983a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "# Calculating the maximum length of among all the sentences which will be useful for padding.\n",
    "max_length_encoder = train['input'].map(len).max()\n",
    "\n",
    "print(max_length_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1M6VC2ixr6J",
    "outputId": "a9d651ee-ad59-412a-9535-f849642c96dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "max_length_decoder = max( train['target_ip'].map(len).max(), train['target_op'].map(len).max())\n",
    "print(max_length_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "brw3mDkGxemO"
   },
   "outputs": [],
   "source": [
    "# Tokenizer for the raw input and target output\n",
    "tokenizer_raw_ip = Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=False,\n",
    "    filters=None\n",
    ")\n",
    "\n",
    "tokenizer_target_ip = Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=False,\n",
    "    filters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "B6VhrB0BxkS_"
   },
   "outputs": [],
   "source": [
    "tokenizer_raw_ip.fit_on_texts(train['input'].values)\n",
    "tokenizer_target_ip.fit_on_texts(train['target_ip'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_OMsgwWd72wz"
   },
   "outputs": [],
   "source": [
    "# Replacing the vocabulary of the trained index to a vocabulary mentioned in the research paper\n",
    "tokenizer_target_ip.word_index = vocabulary\n",
    "tokenizer_raw_ip.word_index = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCobtx0TDkqS",
    "outputId": "8c0ef8c4-e9cf-4026-e2c9-0a57910f1233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "target_vocab_size=len(tokenizer_target_ip.word_index.keys())\n",
    "print(target_vocab_size)\n",
    "input_vocab_size=len(tokenizer_raw_ip.word_index.keys())\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "NbS-j6zFDZTk"
   },
   "outputs": [],
   "source": [
    "# Encoder class with Embedding layer and LSTM layer.\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, name=\"Embedding_Layer_Encoder\")\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, name=\"Encoder_LSTM\")\n",
    "        # self.lstm_2 = LSTM(self.enc_units, return_state=True, name=\"Encoder_LSTM_2\")\n",
    "        # self.lstm_3 = LSTM(self.enc_units, return_state=True, name=\"Encoder_LSTM_3\")\n",
    "        # self.lstm_4 = LSTM(self.enc_units, return_state=True, name=\"Encoder_LSTM_4\")\n",
    "\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        \n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_sentances)\n",
    "#         self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm_2(self.lstm_output)\n",
    "#         self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm_3(self.lstm_output)\n",
    "#         self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm_4(self.lstm_output)\n",
    "\n",
    "\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n",
    "    \n",
    "# Decoder class with embedding and LSTM layer.    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "        # we are using embedding_matrix and not training the embedding layer\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, name=\"Embedding_Layer_Decoder\",)\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "        # self.lstm_2 = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM_2\")\n",
    "        # self.lstm_3 = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM_3\")\n",
    "        # self.lstm_4 = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM_4\")\n",
    "    \n",
    "    def call(self, target_sentences, state_h, state_c):\n",
    "        \n",
    "        lstm_output, _,_        = self.lstm(target_sentences, initial_state=[state_h, state_c])\n",
    "        # lstm_output, _,_        = self.lstm_2(lstm_output, initial_state=[state_h, state_c])\n",
    "        # lstm_output, _,_        = self.lstm_3(lstm_output, initial_state=[state_h, state_c])\n",
    "        # lstm_output, _,_        = self.lstm_4(lstm_output, initial_state=[state_h, state_c])\n",
    "\n",
    "        return lstm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bdPsfdMTDcUa"
   },
   "outputs": [],
   "source": [
    "# Creating a data pipeline\n",
    "class Dataset:\n",
    "    def __init__(self, data, tokenizer_raw_ip, tokenizer_target_ip, max_length_encoder,max_length_decoder):\n",
    "        self.encoder_inps = data['input'].values\n",
    "        self.decoder_inps = data['target_ip'].values\n",
    "        self.decoder_outs = data['target_op'].values\n",
    "        self.tokenizer_target_ip = tokenizer_target_ip\n",
    "        self.tokenizer_raw_ip = tokenizer_raw_ip\n",
    "        self.max_length_encoder = max_length_encoder\n",
    "        self.max_length_decoder = max_length_decoder\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tokenizer_raw_ip.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tokenizer_target_ip.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tokenizer_target_ip.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_length_encoder, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_length_decoder, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_length_decoder, dtype='int32', padding='post')\n",
    "\n",
    "        self.encoder_seq = tf.keras.utils.to_categorical(self.encoder_seq, num_classes=len(tokenizer_raw_ip.word_index.keys())+1)\n",
    "        self.decoder_inp_seq = tf.keras.utils.to_categorical(self.decoder_inp_seq, num_classes=len(tokenizer_target_ip.word_index.keys())+1)\n",
    "        self.decoder_out_seq = tf.keras.utils.to_categorical(self.decoder_out_seq, num_classes=len(tokenizer_target_ip.word_index.keys())+1)\n",
    "\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "            \n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9dmaj0EADi5a"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer_raw_ip, tokenizer_target_ip, max_length_encoder, max_length_decoder)\n",
    "test_dataset  = Dataset(test, tokenizer_raw_ip, tokenizer_target_ip, max_length_encoder, max_length_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ti9VvFy7YE8a",
    "outputId": "bef6f204-4bf6-4b77-8a0c-2c04f1f74d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 161, 97) (64, 200, 97) (64, 200, 97)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=20)\n",
    "\n",
    "print(train_dataloader[1][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TSWOLZxfES-T"
   },
   "outputs": [],
   "source": [
    "# Model 1 - 1 layer LSTM model for each encoder and decoder\n",
    "class Model1(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.encoder = Encoder(vocab_size=input_vocab_size+1, embedding_dim=50, input_length=encoder_inputs_length, enc_units=100)\n",
    "        self.decoder = Decoder(vocab_size=target_vocab_size+1, embedding_dim=50, input_length=decoder_inputs_length, dec_units=100)\n",
    "        self.dense   = Dense(output_vocab_size+1, activation='softmax')\n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "JdhEtfL4mpIo"
   },
   "outputs": [],
   "source": [
    "# Reduce learning rate based on the validation loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.99, verbose=1, mode='min', min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pJCvd660V1we"
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'model_1'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "h54iCsvQElOp"
   },
   "outputs": [],
   "source": [
    "model  = Model1(encoder_inputs_length=max_length_encoder,decoder_inputs_length=max_length_decoder,output_vocab_size=target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AuSPsHDFvhI"
   },
   "outputs": [],
   "source": [
    "# Do once normal loss function works\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def custom_lossfunction(real, pred):\n",
    "\n",
    "  # Custom loss function that will not consider the loss for padded zeros.\n",
    "  # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "  \n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Bcix47d-L2Ap"
   },
   "outputs": [],
   "source": [
    "# Using Adam and Gradient clipping to prevent gradient explosion as mentioned in the research paper\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy')\n",
    "train_steps=train.shape[0]//64\n",
    "valid_steps=test.shape[0]//20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aV6gpE1iE5a7",
    "outputId": "0744ca70-a22f-40a9-b9e7-f16bb940a3e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 4s 60ms/step - loss: 3.7757 - val_loss: 1.3952\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.6538 - val_loss: 1.0314\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.2648 - val_loss: 0.9008\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1970 - val_loss: 0.8636\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1760 - val_loss: 0.8459\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.1676 - val_loss: 0.8293\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1295 - val_loss: 0.8153\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1308 - val_loss: 0.7995\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1125 - val_loss: 0.7821\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1086 - val_loss: 0.7621\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0370 - val_loss: 0.7401\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0344 - val_loss: 0.7167\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.9907 - val_loss: 0.6932\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.9835 - val_loss: 0.6717\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.9495 - val_loss: 0.6538\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.9224 - val_loss: 0.6381\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.9268 - val_loss: 0.6254\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.9026 - val_loss: 0.6149\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8798 - val_loss: 0.6061\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.8869 - val_loss: 0.5969\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8629 - val_loss: 0.5910\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.8337 - val_loss: 0.5844\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.8235 - val_loss: 0.5789\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8324 - val_loss: 0.5740\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.8195 - val_loss: 0.5681\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.8158 - val_loss: 0.5628\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8099 - val_loss: 0.5593\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8057 - val_loss: 0.5538\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7877 - val_loss: 0.5503\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7859 - val_loss: 0.5475\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7895 - val_loss: 0.5439\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7810 - val_loss: 0.5409\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7684 - val_loss: 0.5380\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7742 - val_loss: 0.5347\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7681 - val_loss: 0.5311\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7742 - val_loss: 0.5294\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7468 - val_loss: 0.5262\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7648 - val_loss: 0.5249\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7431 - val_loss: 0.5214\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7592 - val_loss: 0.5187\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7359 - val_loss: 0.5166\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7394 - val_loss: 0.5143\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7453 - val_loss: 0.5125\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7454 - val_loss: 0.5097\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7409 - val_loss: 0.5073\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.7385 - val_loss: 0.5063\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7368 - val_loss: 0.5047\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7356 - val_loss: 0.5013\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7071 - val_loss: 0.5017\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7185 - val_loss: 0.4990\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7066 - val_loss: 0.4971\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7195 - val_loss: 0.4949\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7194 - val_loss: 0.4929\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.7077 - val_loss: 0.4918\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7057 - val_loss: 0.4893\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7045 - val_loss: 0.4878\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.7014 - val_loss: 0.4876\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6971 - val_loss: 0.4844\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 0.6936 - val_loss: 0.4838\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6957 - val_loss: 0.4811\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6833 - val_loss: 0.4817\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.7081 - val_loss: 0.4799\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6983 - val_loss: 0.4771\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6821 - val_loss: 0.4763\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6887 - val_loss: 0.4753\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6822 - val_loss: 0.4732\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.6660 - val_loss: 0.4717\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6682 - val_loss: 0.4708\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6802 - val_loss: 0.4706\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6770 - val_loss: 0.4684\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6813 - val_loss: 0.4677\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6685 - val_loss: 0.4649\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6532 - val_loss: 0.4650\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6575 - val_loss: 0.4616\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.6659 - val_loss: 0.4606\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6719 - val_loss: 0.4593\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6446 - val_loss: 0.4601\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.6481 - val_loss: 0.4573\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6548 - val_loss: 0.4555\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6560 - val_loss: 0.4545\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6506 - val_loss: 0.4537\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6525 - val_loss: 0.4537\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6629 - val_loss: 0.4511\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6511 - val_loss: 0.4503\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6433 - val_loss: 0.4497\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6448 - val_loss: 0.4489\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6698 - val_loss: 0.4478\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6260 - val_loss: 0.4470\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6542 - val_loss: 0.4440\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6356 - val_loss: 0.4456\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6380 - val_loss: 0.4437\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6401 - val_loss: 0.4428\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6352 - val_loss: 0.4412\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6356 - val_loss: 0.4409\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6454 - val_loss: 0.4398\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6369 - val_loss: 0.4396\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6356 - val_loss: 0.4366\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6334 - val_loss: 0.4363\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6147 - val_loss: 0.4361\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6343 - val_loss: 0.4332\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6168 - val_loss: 0.4331\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6154 - val_loss: 0.4316\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6236 - val_loss: 0.4298\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.6158 - val_loss: 0.4297\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.6043 - val_loss: 0.4277\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6105 - val_loss: 0.4246\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6030 - val_loss: 0.4240\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5998 - val_loss: 0.4230\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6043 - val_loss: 0.4218\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6062 - val_loss: 0.4207\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5975 - val_loss: 0.4188\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.6121 - val_loss: 0.4172\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5867 - val_loss: 0.4177\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.6019 - val_loss: 0.4177\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5953 - val_loss: 0.4149\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5926 - val_loss: 0.4163\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5831 - val_loss: 0.4153\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5986 - val_loss: 0.4149\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5826 - val_loss: 0.4125\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5793 - val_loss: 0.4128\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5873 - val_loss: 0.4121\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5945 - val_loss: 0.4097\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5792 - val_loss: 0.4105\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5889 - val_loss: 0.4079\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5887 - val_loss: 0.4081\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5840 - val_loss: 0.4069\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5794 - val_loss: 0.4070\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5790 - val_loss: 0.4074\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5850 - val_loss: 0.4059\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5670 - val_loss: 0.4054\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5690 - val_loss: 0.4049\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5754 - val_loss: 0.4051\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5777 - val_loss: 0.4026\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5675 - val_loss: 0.4028\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5727 - val_loss: 0.4029\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5671 - val_loss: 0.4023\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5725 - val_loss: 0.4005\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5666 - val_loss: 0.4012\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5590 - val_loss: 0.4014\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5728 - val_loss: 0.3990\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5649 - val_loss: 0.3978\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5503 - val_loss: 0.3988\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5554 - val_loss: 0.4003\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5651 - val_loss: 0.3978\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5638 - val_loss: 0.3961\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5613 - val_loss: 0.3987\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5487 - val_loss: 0.3968\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5625 - val_loss: 0.3962\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5518 - val_loss: 0.3953\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5378 - val_loss: 0.3944\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5610 - val_loss: 0.3941\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5584 - val_loss: 0.3951\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5443 - val_loss: 0.3936\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5297 - val_loss: 0.3925\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5410 - val_loss: 0.3914\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5614 - val_loss: 0.3902\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5516 - val_loss: 0.3899\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5456 - val_loss: 0.3914\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5481 - val_loss: 0.3909\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5351 - val_loss: 0.3894\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5472 - val_loss: 0.3887\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5357 - val_loss: 0.3897\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5331 - val_loss: 0.3890\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5415 - val_loss: 0.3876\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5471 - val_loss: 0.3866\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5200 - val_loss: 0.3883\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5358 - val_loss: 0.3873\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5404 - val_loss: 0.3855\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5272 - val_loss: 0.3857\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5342 - val_loss: 0.3870\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5398 - val_loss: 0.3866\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5280 - val_loss: 0.3863\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5421 - val_loss: 0.3853\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5398 - val_loss: 0.3841\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5226 - val_loss: 0.3845\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5215 - val_loss: 0.3843\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5238 - val_loss: 0.3850\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5342 - val_loss: 0.3839\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5288 - val_loss: 0.3823\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5288 - val_loss: 0.3828\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5387 - val_loss: 0.3847\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5212 - val_loss: 0.3833\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5328 - val_loss: 0.3819\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5196 - val_loss: 0.3816\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5216 - val_loss: 0.3822\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5079 - val_loss: 0.3843\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5214 - val_loss: 0.3822\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 0.5121 - val_loss: 0.3808\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5169 - val_loss: 0.3804\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5167 - val_loss: 0.3822\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5007 - val_loss: 0.3832\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5153 - val_loss: 0.3804\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5172 - val_loss: 0.3803\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5150 - val_loss: 0.3803\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5206 - val_loss: 0.3798\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5128 - val_loss: 0.3794\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5081 - val_loss: 0.3807\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5127 - val_loss: 0.3789\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5016 - val_loss: 0.3785\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5174 - val_loss: 0.3796\n",
      "Model: \"model1_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_4 (Encoder)          multiple                  79200     \n",
      "_________________________________________________________________\n",
      "decoder_4 (Decoder)          multiple                  79200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  9797      \n",
      "=================================================================\n",
      "Total params: 168,197\n",
      "Trainable params: 168,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, steps_per_epoch=train_steps, epochs=200,  validation_data=test_dataloader, validation_steps=valid_steps, callbacks=[reduce_lr, model_checkpoint_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXHxPv5kb-R4",
    "outputId": "645e7952-c48c-4be6-85e8-bf4a03eaae8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5117 - val_loss: 0.3777\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5104 - val_loss: 0.3776\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5099 - val_loss: 0.3776\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5092 - val_loss: 0.3773\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5090 - val_loss: 0.3757\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5083 - val_loss: 0.3774\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5071 - val_loss: 0.3756\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5059 - val_loss: 0.3764\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.5056 - val_loss: 0.3765\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5050 - val_loss: 0.3764\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5040 - val_loss: 0.3760\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5032 - val_loss: 0.3751\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.5024 - val_loss: 0.3770\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5022 - val_loss: 0.3754\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5014 - val_loss: 0.3760\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5009 - val_loss: 0.3766\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.5001 - val_loss: 0.3746\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4991 - val_loss: 0.3745\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4985 - val_loss: 0.3746\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4979 - val_loss: 0.3763\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4978 - val_loss: 0.3744\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4968 - val_loss: 0.3760\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4961 - val_loss: 0.3748\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4951 - val_loss: 0.3739\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4946 - val_loss: 0.3747\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4938 - val_loss: 0.3747\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4930 - val_loss: 0.3741\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4927 - val_loss: 0.3747\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4924 - val_loss: 0.3743\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4916 - val_loss: 0.3745\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4909 - val_loss: 0.3727\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4899 - val_loss: 0.3727\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4894 - val_loss: 0.3751\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4888 - val_loss: 0.3739\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4881 - val_loss: 0.3756\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4876 - val_loss: 0.3727\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4867 - val_loss: 0.3737\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4870 - val_loss: 0.3746\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4857 - val_loss: 0.3727\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4850 - val_loss: 0.3741\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4847 - val_loss: 0.3724\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4841 - val_loss: 0.3741\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4831 - val_loss: 0.3730\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4829 - val_loss: 0.3737\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4821 - val_loss: 0.3727\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4819 - val_loss: 0.3746\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4816 - val_loss: 0.3724\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4804 - val_loss: 0.3748\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4798 - val_loss: 0.3733\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4791 - val_loss: 0.3741\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4785 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0009900000470224768.\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4779 - val_loss: 0.3742\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4779 - val_loss: 0.3720\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4766 - val_loss: 0.3733\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4760 - val_loss: 0.3739\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4754 - val_loss: 0.3722\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4750 - val_loss: 0.3736\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4745 - val_loss: 0.3732\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4742 - val_loss: 0.3740\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4735 - val_loss: 0.3740\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4728 - val_loss: 0.3738\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4721 - val_loss: 0.3748\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4717 - val_loss: 0.3738\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.000980100086890161.\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4709 - val_loss: 0.3724\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4707 - val_loss: 0.3719\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4702 - val_loss: 0.3743\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4692 - val_loss: 0.3738\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4686 - val_loss: 0.3735\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4681 - val_loss: 0.3760\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4677 - val_loss: 0.3720\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4672 - val_loss: 0.3734\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4665 - val_loss: 0.3713\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4659 - val_loss: 0.3719\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4656 - val_loss: 0.3722\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4651 - val_loss: 0.3721\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4652 - val_loss: 0.3734\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4644 - val_loss: 0.3711\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4636 - val_loss: 0.3737\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4631 - val_loss: 0.3729\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4625 - val_loss: 0.3728\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4622 - val_loss: 0.3728\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4615 - val_loss: 0.3720\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4612 - val_loss: 0.3763\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4608 - val_loss: 0.3724\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4597 - val_loss: 0.3727\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4594 - val_loss: 0.3743\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4584 - val_loss: 0.3757\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0009702991275116801.\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4584 - val_loss: 0.3756\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4577 - val_loss: 0.3753\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4569 - val_loss: 0.3716\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4575 - val_loss: 0.3743\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4569 - val_loss: 0.3731\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4560 - val_loss: 0.3736\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4551 - val_loss: 0.3742\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4543 - val_loss: 0.3756\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4545 - val_loss: 0.3751\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4542 - val_loss: 0.3746\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0009605961316265165.\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4534 - val_loss: 0.3740\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4533 - val_loss: 0.3764\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4524 - val_loss: 0.3769\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4518 - val_loss: 0.3775\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4512 - val_loss: 0.3770\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4506 - val_loss: 0.3789\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4505 - val_loss: 0.3772\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4497 - val_loss: 0.3767\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4492 - val_loss: 0.3775\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4486 - val_loss: 0.3764\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0009509901772253215.\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4488 - val_loss: 0.3782\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4489 - val_loss: 0.3768\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4475 - val_loss: 0.3762\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4471 - val_loss: 0.3769\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4463 - val_loss: 0.3770\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4459 - val_loss: 0.3743\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4456 - val_loss: 0.3785\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4454 - val_loss: 0.3733\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4448 - val_loss: 0.3775\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4444 - val_loss: 0.3793\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 0.0009414802846731617.\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4434 - val_loss: 0.3778\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4431 - val_loss: 0.3818\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4427 - val_loss: 0.3777\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4427 - val_loss: 0.3766\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4424 - val_loss: 0.3789\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4418 - val_loss: 0.3769\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4409 - val_loss: 0.3774\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4402 - val_loss: 0.3808\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4402 - val_loss: 0.3805\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4403 - val_loss: 0.3791\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 0.0009320654743351042.\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4395 - val_loss: 0.3771\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4384 - val_loss: 0.3791\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4382 - val_loss: 0.3788\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4379 - val_loss: 0.3787\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4373 - val_loss: 0.3783\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4373 - val_loss: 0.3765\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4369 - val_loss: 0.3800\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4359 - val_loss: 0.3819\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4359 - val_loss: 0.3799\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4354 - val_loss: 0.3786\n",
      "\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 0.0009227448242017999.\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4349 - val_loss: 0.3804\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4348 - val_loss: 0.3828\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4342 - val_loss: 0.3820\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4335 - val_loss: 0.3795\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4332 - val_loss: 0.3819\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4326 - val_loss: 0.3802\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4323 - val_loss: 0.3800\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4318 - val_loss: 0.3816\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4312 - val_loss: 0.3807\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4316 - val_loss: 0.3845\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.0009135173546383158.\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4309 - val_loss: 0.3827\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4303 - val_loss: 0.3830\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4300 - val_loss: 0.3847\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4294 - val_loss: 0.3833\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4298 - val_loss: 0.3803\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4288 - val_loss: 0.3837\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4280 - val_loss: 0.3830\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4278 - val_loss: 0.3828\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4275 - val_loss: 0.3848\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4268 - val_loss: 0.3858\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.0009043822012608871.\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4267 - val_loss: 0.3844\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4259 - val_loss: 0.3848\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4259 - val_loss: 0.3855\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4255 - val_loss: 0.3843\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4251 - val_loss: 0.3845\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4246 - val_loss: 0.3851\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4240 - val_loss: 0.3850\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4241 - val_loss: 0.3875\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4236 - val_loss: 0.3867\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4234 - val_loss: 0.3837\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 0.0008953383844345808.\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4225 - val_loss: 0.3845\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4217 - val_loss: 0.3851\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4214 - val_loss: 0.3839\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4215 - val_loss: 0.3864\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4211 - val_loss: 0.3885\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4209 - val_loss: 0.3893\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4201 - val_loss: 0.3881\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4199 - val_loss: 0.3917\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4199 - val_loss: 0.3928\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4191 - val_loss: 0.3894\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.000886384982150048.\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4184 - val_loss: 0.3890\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4180 - val_loss: 0.3869\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4186 - val_loss: 0.3876\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4182 - val_loss: 0.3883\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4177 - val_loss: 0.3878\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4167 - val_loss: 0.3887\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4164 - val_loss: 0.3906\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4167 - val_loss: 0.3900\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4157 - val_loss: 0.3901\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4155 - val_loss: 0.3899\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 0.0008775211300235242.\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4149 - val_loss: 0.3908\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4149 - val_loss: 0.3910\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4147 - val_loss: 0.3920\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4141 - val_loss: 0.3932\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.4140 - val_loss: 0.3923\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4131 - val_loss: 0.3918\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4136 - val_loss: 0.3902\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4129 - val_loss: 0.3929\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4123 - val_loss: 0.3944\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4119 - val_loss: 0.3943\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.0008687459060456604.\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.4111 - val_loss: 0.3928\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4107 - val_loss: 0.3882\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.4109 - val_loss: 0.3938\n",
      "Model: \"model1_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_4 (Encoder)          multiple                  79200     \n",
      "_________________________________________________________________\n",
      "decoder_4 (Decoder)          multiple                  79200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  9797      \n",
      "=================================================================\n",
      "Total params: 168,197\n",
      "Trainable params: 168,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, steps_per_epoch=train_steps, epochs=200,  validation_data=test_dataloader, validation_steps=valid_steps, callbacks=[reduce_lr, model_checkpoint_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3YU1I_PeuTu",
    "outputId": "cfbd8db0-2225-4d74-c2ee-20b0cc742b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe2a0025450>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f3mD2ytpkGS",
    "outputId": "d845a5a0-2ad5-4d3b-d697-5b1a7088ef91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.371053010225296"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZH9LomGye4l"
   },
   "outputs": [],
   "source": [
    "model.save_weights('./model_1/model_novel_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaQQNo8EJD3h",
    "outputId": "07aa25ce-ba03-448b-861c-720b01891e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss of the model 1 is: 0.3711\n",
      "The perplexity of the model 1 is: 1.2933385761793754\n"
     ]
    }
   ],
   "source": [
    "print(\"The validation loss of the model 1 is:\", 0.3711)\n",
    "print(\"The perplexity of the model 1 is:\", 2**(0.3711))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "B8S2iljez9AW"
   },
   "outputs": [],
   "source": [
    "start_index = tokenizer_target_ip.word_index['\\t']\n",
    "end_index = tokenizer_target_ip.word_index['\\n']\n",
    "DECODER_SEQ_LEN = max_length_decoder\n",
    "max_len = max_length_decoder\n",
    "\n",
    "\n",
    "def predict(input_sentence):\n",
    "\n",
    "  encoder_seq = tokenizer_raw_ip.texts_to_sequences([input_sentence])\n",
    "\n",
    "  encoder_seq = pad_sequences(encoder_seq, maxlen=max_length_encoder, dtype='int32', padding='post')\n",
    "\n",
    "  encoder_seq = tf.keras.utils.to_categorical(encoder_seq, num_classes=len(tokenizer_raw_ip.word_index.keys())+1)\n",
    "\n",
    "  enc_output, enc_state_h, enc_state_c = model.layers[0](encoder_seq)\n",
    "  \n",
    "  # enc_output, enc_state_h, enc_state_c = model.layers[0].lstm_2(enc_output)\n",
    "  # enc_output, enc_state_h, enc_state_c = model.layers[0].lstm_3(enc_output)\n",
    "  # enc_output, enc_state_h, enc_state_c = model.layers[0].lstm_4(enc_output)\n",
    "\n",
    "  dec_input = np.zeros((1, 1, len(tokenizer_raw_ip.word_index.keys())+1))\n",
    "\n",
    "  dec_input[0, 0, tokenizer_target_ip.word_index['\\t']] = 1.\n",
    "\n",
    "  input_state = [enc_state_h, enc_state_c]\n",
    "\n",
    "  output_word = []\n",
    "\n",
    "  for i in range(DECODER_SEQ_LEN):\n",
    "      # cur_emb = model.layers[1].embedding(dec_input)\n",
    "\n",
    "      predicted_out, state_h, state_c = model.layers[1].lstm(dec_input, input_state)\n",
    "\n",
    "      # predicted_out, state_h, state_c = model.layers[1].lstm_2(predicted_out, input_state)\n",
    "      # predicted_out, state_h, state_c = model.layers[1].lstm_3(predicted_out, input_state)\n",
    "      # predicted_out, state_h, state_c = model.layers[1].lstm_4(predicted_out, input_state)\n",
    "\n",
    "      dense_layer_out = model.layers[2](predicted_out)\n",
    "\n",
    "      input_state = [state_h, state_c]\n",
    "  \n",
    "      output_word_index = np.argmax(dense_layer_out)\n",
    "\n",
    "      # print(output_word_index)\n",
    "\n",
    "      for key, value in tokenizer_target_ip.word_index.items():\n",
    "\n",
    "         if output_word_index == value:\n",
    "             output_word.append(key)\n",
    "\n",
    "      dec_input = np.reshape(output_word_index, (1, 1))\n",
    "\n",
    "      dec_input = np.zeros((1, 1, len(tokenizer_raw_ip.word_index.keys())+1))\n",
    "\n",
    "      dec_input[0, 0, output_word_index] = 1.\n",
    "\n",
    "\n",
    "      if output_word_index == tokenizer_target_ip.word_index['\\n']:\n",
    "        break\n",
    "  return output_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQdlkAgYz9AZ",
    "outputId": "3da0840a-869c-42ee-d16f-21a8e930ffd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Mmm thats better now i got a roast down me! i'd b better if i had a few drinks down me 2! Good indian?\n",
      "Predicted Sentence: Hey, I want to go to chat with you to stay only to chat with you to come to come at 1:30pm. I haven't see you all the show?\n",
      "\n",
      "Original English sentence: That's better now, I got a roast down me! I'd be better if I had a few drinks down me too! Good Indian?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Watch wat?\n",
      "Predicted Sentence: Haha. I am not going to see you.\n",
      "\n",
      "Original English sentence: Watch what?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Oh dat.hehe.Why r u so interested?\n",
      "Predicted Sentence: Hey, I don't know where are you free tomorrow?\n",
      "\n",
      "Original English sentence: Oh that. Hehe. Why are you so interested?\n",
      "\n",
      "******************************\n",
      "Input Sentence: hai\n",
      "Predicted Sentence: Hey.\n",
      "\n",
      "Original English sentence: Hi.\n",
      "\n",
      "******************************\n",
      "Input Sentence: ask more abt me?\n",
      "Predicted Sentence: Hey, I want to go to see you.\n",
      "\n",
      "Original English sentence: Ask more about me?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Hey jiayin. Can bring 10 bucks tmr, it's 4 wawa's bdae.\n",
      "Predicted Sentence: Hey, I don't know where are you free to chat with you to come to come at 12:30.\n",
      "\n",
      "Original English sentence: Hey Jiayin. Can you bring 10 bucks tomorrow? It's for Wawa's birthday.\n",
      "\n",
      "******************************\n",
      "Input Sentence: Wanna intro.Joey?\n",
      "Predicted Sentence: Hey, I want to go to see you.\n",
      "\n",
      "Original English sentence: Want to introduce, Joey?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Hey call me when you are abt to reach? I'm going muji to see see look look\n",
      "Predicted Sentence: Hey, I don't know where are you free to chat with you to come to come at 1:30pm.\n",
      "\n",
      "Original English sentence: Hey, call me when you are about to reach? I'm going to muji to have a look.\n",
      "\n",
      "******************************\n",
      "Input Sentence: Contraction line .\n",
      "Predicted Sentence: Hey, I want to go to chat?\n",
      "\n",
      "Original English sentence: Contraction line.\n",
      "\n",
      "******************************\n",
      "Input Sentence: He say dun tink they need part timer.How? U go crepes n cream ask la.Hereen 1.\n",
      "Predicted Sentence: Hey, I want to go to chat with you to stay only to chat with you to come to come at 12:30.\n",
      "\n",
      "Original English sentence: He says he doesn't think they need part timer. How? Go Creps and Cream ask. The one in Hereen.\n",
      "\n",
      "******************************\n",
      "Input Sentence: Can i come in half an hr later. I nd to bath.Gee.\n",
      "Predicted Sentence: Hey, I want to go to chat with you to stay only to chat?\n",
      "\n",
      "Original English sentence: Can I come in half an hour later? I need to bath.\n",
      "\n",
      "******************************\n",
      "Input Sentence: Wat u doing?\n",
      "Predicted Sentence: Hey, you are going to see you.\n",
      "\n",
      "Original English sentence: What are you doing?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Once i pick up it'll just cut off. SBS or something? are you a member of something? :)\n",
      "Predicted Sentence: Hey, I want to go to chat with you to stay only to chat with you to come to come at 1:30pm. I haven't see you?\n",
      "\n",
      "Original English sentence: Once I pick up it'll just cut off. SBS or something? Are you a member of something?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Hey u will be in orchard right? I'm going to ask mei they all if they've bought the frame. If not we go ikea and buy one? Coz v few pieces left.can complete\n",
      "Predicted Sentence: Hey do you want to go to chat with you to stay only to chat with you to come to come at 1:30pm. I haven't see you all the bus already. I am not studying to see you to stay only.\n",
      "\n",
      "Original English sentence: Hey you will be in Orchard right? I'm going to ask Mei and the rest if they've bought the frame. If not we'll go to Ikea and buy one? Because there are very few pieces left. Can complete.\n",
      "\n",
      "******************************\n",
      "Input Sentence: But cun lah. go next wk? Act where u wanna go?\n",
      "Predicted Sentence: Hey, I want to go to chat with you to stay only to chat?\n",
      "\n",
      "Original English sentence: But please come. Are you going next week? Ask you where you want to go.\n",
      "\n",
      "******************************\n",
      "Input Sentence: ok!\n",
      "Predicted Sentence: Hey.\n",
      "\n",
      "Original English sentence: Ok!\n",
      "\n",
      "******************************\n",
      "Input Sentence: So where n wat time u wan meet?\n",
      "Predicted Sentence: Hey, I want to go to chat with you.\n",
      "\n",
      "Original English sentence: So where and what time do you want to meet?\n",
      "\n",
      "******************************\n",
      "Input Sentence: You want a not? I will go buy for you. I reached orchard already\n",
      "Predicted Sentence: Hey, I don't know where are you free to chat with you to come to come along with you.\n",
      "\n",
      "Original English sentence: You want or not? I will go to buy for you. I reached Orchard already.\n",
      "\n",
      "******************************\n",
      "Input Sentence: Hi, u male\n",
      "Predicted Sentence: Hey.\n",
      "\n",
      "Original English sentence: Hi, are you male?\n",
      "\n",
      "******************************\n",
      "Input Sentence: Where are you\n",
      "Predicted Sentence: Hey.\n",
      "\n",
      "Original English sentence: Where are you?\n",
      "\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "predicted_sentences = []\n",
    "actual_sentences = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    output = predict(row['input'])\n",
    "    predicted_sentences.append(output)\n",
    "    english_out = row['target_op'].split()\n",
    "    actual_sentences.append(english_out)\n",
    "    sentence = ''.join(output)\n",
    "    print('Input Sentence:',row['input'])\n",
    "    print('Predicted Sentence:',sentence)\n",
    "    print('Original English sentence:', row['target_op'])\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5bNFQyuz9Aa",
    "outputId": "d816b61b-55bc-4ae6-d889-dc282176c083"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "avg_score = 0\n",
    "for i in range(len(actual_sentences)):\n",
    "    score = sentence_bleu([actual_sentences[i]], predicted_sentences[i])\n",
    "    avg_score += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yUCr-G-z9Aa",
    "outputId": "b94d099b-c920-40f5-81b7-bd88c92dce39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg BLEU Score of Simple Encoder Decoder Model: 0.09079140153883539\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg BLEU Score of Simple Encoder Decoder Model:\", (avg_score/(len(actual_sentences))))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS2_Modelling_ohe (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
