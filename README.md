# Sentence Correction using RNN

## Models Used : Encoder-Decoder Models, Luong Attenion Model, Transformers.
- This Case Study on “Sentence Correction using Recurrent Neural Networks” is a research paper-based case study dealing with mapping text which have social media type of shortened texts to a proper English language text. The paper mainly deals with preprocessing of the raw text obtained from a public domain and converting it to texts which are close to English language. For example: “Lea u there?” should be converted to “Lea, are you there?”.
The paper starts with explaining various existing models such as Luong Attention, Hidden Markov model, Bahdanau Models which mainly use word correction and this paper tried to implement a character level approach.
- Paper Approach: The main approach of the research paper is one hot encoding the characters into a 94-dimension vector. 94 dimensions is chosen since there are 94 printable/visible ASCII characters. After embedding the character into a 94D vector, it is passed onto RNNs. RNNs are chosen since they are  very effective in capturing sequential data and also capture sophisticated interactions.
The research paper mainly stays with 1 and 2 layers of RNNs but also mentions the fact that deeper the model, the model can learn more natural concepts beyond character dynamics such as word level tokens, named entity recognitions and also correct simple grammar. Among the RNNs the paper uses LSTMs since at character level, the Gating will be useful as the number of characters will be more compared to the number of words. At the output layer a SoftMax function is used to decide the final output character.
Standard Mini Batch stochastic gradient descent (mini batch SGD) is used as the training algorithm along with cross entropy as the loss function.
The dataset used in the entire process is from the National University of Singapore which contains 2000 texts of normalized text. Initially the histograms of the lengths of source texts and target sentences are observed which suggests that there are very sparse sentences with length greater than 170 and 200 in the sets, respectively. Further, as part of preprocessing the sentences with non-printable ASCII characters and sentence pairs with greater than length 170 and 200 in source and target sets are removed. Overall, after the two preprocessing steps, 11 sentence pairs were dropped from the dataset. The dataset was already randomized and hence no randomization is further required. Since the dataset is very small, train and test split of 99:1 is used.
- There are two types of model discussed in the research paper namely,
1.   	Baseline Model:
- For the baseline model, a character level unigram is used where each line is mapped character by character to its respective line in the target set. A single character has a dictionary of all characters with their respective counts in the target set. To generate output translations, a one source character is chosen at one time, using the dictionary of that character, we find the next character by selecting the character with the highest count.
Similarly, word level unigram is used for word level approach. Capitalization and Punctuation were not removed during the training. For both models, the loss for the validation set was at 5.366 and 2.009 for character level and word level, respectively.
2.   	Neural Translation Model – Main Approach of the research paper.
Neural machines were trained with 8000 steps, batch size 64, 260 epochs, learning rate of 0.5 with decay rate of 0.99, maximum gradient norm of 5 and validation at every 100 epochs and hidden state size of 100. The hidden state of 100 was chosen to comply with the similar 94D input vector.
Two models one with 1-layer and second with 2-layer LSTMs were used.  The 1-layer provided a loss score of 1.060 and 2-layer of 1.102.
There was another method tried in order to show why preprocessing of text is needed before feeding the model with raw input. The raw input text was fed into a NMT as shown in the second model. The output obtained from NMT was given as input to the Baseline models and the results were improved compared to only Baseline models. For 1-layer NMT+Base Line model the loss was 5.014 and 1.533 for char and word level. For 2-layer the loss was 5.278 and 1.490 for char and word level, respectively. Thus, we see both the losses decreasing from only using the baseline models. The word level loss reduction is more significant over the character level losses.
- Results: Both the models were able to correct basic spelling errors (tink -> think), basic grammatical corrections (sentence begin with capitals), long and short abbreviations (u -> you) and also preserve text when no errors were found.
- Conclusions: The dataset was pretty small to get a higher accuracy. With more layers and more data significant improvements and predictions can be made. a 1-layer performed better than a 2-layer model since a 2-layer model usually requires a greater number of cycles to learn and converge on the optimum point. A 2-layer model learns better and higher-level corrections such as punctuations which the 1-layer model fails to learn.

